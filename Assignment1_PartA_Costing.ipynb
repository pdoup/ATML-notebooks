{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "costing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Cost-Sensitive Learning on Statlog(Heart) dataset\n",
        "\n",
        "### Panagiotis Doupidis, DWS 89\n",
        "[Link to dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat)\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XRKmhj8IX7WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==0.22.2 costcla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W00EF0YYenCx",
        "outputId": "dacbd667-ab14-4366-d5ff-f09a976c8bd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.22.2\n",
            "  Downloading scikit_learn-0.22.2-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting costcla\n",
            "  Downloading costcla-0.6-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 30.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2) (1.1.0)\n",
            "Collecting pyea>=0.2\n",
            "  Downloading pyea-0.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: pandas>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from costcla) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14.0->costcla) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14.0->costcla) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.14.0->costcla) (1.15.0)\n",
            "Building wheels for collected packages: pyea\n",
            "  Building wheel for pyea (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyea: filename=pyea-0.2-py3-none-any.whl size=6018 sha256=7c830b3b4e6228e79b3ae1a811a6c99210e49ce0602497900df87837dedfc05b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/c7/f9/c43bd31860d7235d875091659066bf793ea300fd0621156737\n",
            "Successfully built pyea\n",
            "Installing collected packages: scikit-learn, pyea, costcla\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2 which is incompatible.\u001b[0m\n",
            "Successfully installed costcla-0.6 pyea-0.2 scikit-learn-0.22.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iiREnhzqXcoF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import the dataset and transform the response (heart_disease) to 0 (absence) and 1 (presence)"
      ],
      "metadata": {
        "id": "1eLaCny_cCNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "header_ = ['age', 'sex', 'chest_pain', 'resting_bp', 'cholesterol', 'blood_sugar', \n",
        "           'resting_ekg', 'max_hr', 'angina', 'oldpeak', 'slope_ST', 'no_of_vessels', 'thal', 'heart_disease']\n",
        "\n",
        "# File is fetched from the UCI webpage\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat',\n",
        "                 names=header_, header=None, delim_whitespace=True)\n",
        "\n",
        "'''\n",
        "  subtract 1 from the response variable so that 0 indicates absense and 1\n",
        "  presense of heart condition instead of the original 1 and 2 respectively\n",
        "'''\n",
        "\n",
        "df.heart_disease = df.heart_disease - 1\n",
        "\n",
        "df.heart_disease.value_counts() # 0=absence, 1=presence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_XjolZWYayf",
        "outputId": "9d464029-6fde-441d-c31f-b2633e56ef8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    150\n",
              "1    120\n",
              "Name: heart_disease, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No cost minimization"
      ],
      "metadata": {
        "id": "1SdhuTBqei1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from costcla.metrics import cost_loss\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X, y = df.drop(columns='heart_disease'), df.heart_disease\n",
        "\n",
        "# 75% train, 25% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "fp = np.full((y_test.shape[0],1), 1)\n",
        "fn = np.full((y_test.shape[0],1), 5)\n",
        "tp = np.zeros((y_test.shape[0],1))\n",
        "tn = np.zeros((y_test.shape[0],1))\n",
        "\n",
        "cost_matrix = np.hstack((fp, fn, tp, tn))\n",
        "\n",
        "# rows are the predicted values, columns the actual like in the slides\n",
        "cost_matrix_uci = np.array(((0,5),(1,0)))\n",
        "\n",
        "classifiers = [RandomForestClassifier(n_estimators=100), \n",
        "               SVC(kernel='linear', C=1, probability=True), MultinomialNB()]\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
        "    print(conf_m) \n",
        "    print(f\"Native implementation : {np.sum(conf_m * cost_matrix_uci)}\")\n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print(f\"Using the library function : {loss:.0f}\", end='\\n\\n')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aCO442aYk6y",
        "outputId": "dd6502fb-cb5f-469b-df3c-512e7f5d2905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.892     0.868     0.880        38\n",
            "    presence      0.839     0.867     0.852        30\n",
            "\n",
            "    accuracy                          0.868        68\n",
            "   macro avg      0.865     0.868     0.866        68\n",
            "weighted avg      0.868     0.868     0.868        68\n",
            "\n",
            "[[33  4]\n",
            " [ 5 26]]\n",
            "Native implementation : 25\n",
            "Using the library function : 25\n",
            "\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.943     0.868     0.904        38\n",
            "    presence      0.848     0.933     0.889        30\n",
            "\n",
            "    accuracy                          0.897        68\n",
            "   macro avg      0.896     0.901     0.896        68\n",
            "weighted avg      0.901     0.897     0.897        68\n",
            "\n",
            "[[33  2]\n",
            " [ 5 28]]\n",
            "Native implementation : 15\n",
            "Using the library function : 15\n",
            "\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.838     0.816     0.827        38\n",
            "    presence      0.774     0.800     0.787        30\n",
            "\n",
            "    accuracy                          0.809        68\n",
            "   macro avg      0.806     0.808     0.807        68\n",
            "weighted avg      0.810     0.809     0.809        68\n",
            "\n",
            "[[31  6]\n",
            " [ 7 24]]\n",
            "Native implementation : 37\n",
            "Using the library function : 37\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before applying a cost-minimization method we can deduce that the linear SVM (with default parameters) is the classifier with the least cost on the test set and the Naive Bayes classifer has the worst scores of all three. Here the cost matrix is taken straight from the UCI website and it states that false positives have a cost of 1, where as false negatives a cost of 5. This means that the penalty for predicting an individual with heart disease as healthy is 5 times greater than the opposite. True positives and true negatives have a cost of 0. Also, although we print the classification matrix with the precision, recall and f1 metrics we don't base our decisions on it since we care mostly about the total cost.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u0tCkPrfdgZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to minimize the expected cost using costcla's Minimum Risk Classifier\n",
        "(no calibration)"
      ],
      "metadata": {
        "id": "hMG8ts4Dh9G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from costcla.models import BayesMinimumRiskClassifier\n",
        "import joblib\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# No calibration using Costcla's classifier\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred_prob = clf.predict_proba(X_test)\n",
        "\n",
        "    bmr = BayesMinimumRiskClassifier(calibration=False)\n",
        "    y_pred = bmr.predict(y_pred_prob, cost_matrix)\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
        "    print(conf_m) \n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print(f\"\\nCost : {loss:.0f}\", end='\\n\\n')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "f0mUWvK0oMRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151c048c-9a4b-4c81-fcde-bfaa10bb3b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.938     0.395     0.556        38\n",
            "    presence      0.558     0.967     0.707        30\n",
            "\n",
            "    accuracy                          0.647        68\n",
            "   macro avg      0.748     0.681     0.631        68\n",
            "weighted avg      0.770     0.647     0.623        68\n",
            "\n",
            "[[15  1]\n",
            " [23 29]]\n",
            "\n",
            "Cost : 28\n",
            "\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      1.000     0.526     0.690        38\n",
            "    presence      0.625     1.000     0.769        30\n",
            "\n",
            "    accuracy                          0.735        68\n",
            "   macro avg      0.812     0.763     0.729        68\n",
            "weighted avg      0.835     0.735     0.725        68\n",
            "\n",
            "[[20  0]\n",
            " [18 30]]\n",
            "\n",
            "Cost : 18\n",
            "\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.903     0.737     0.812        38\n",
            "    presence      0.730     0.900     0.806        30\n",
            "\n",
            "    accuracy                          0.809        68\n",
            "   macro avg      0.816     0.818     0.809        68\n",
            "weighted avg      0.827     0.809     0.809        68\n",
            "\n",
            "[[28  3]\n",
            " [10 27]]\n",
            "\n",
            "Cost : 25\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear SVM seems to outperform the other 2 classifiers having the lowest cost. Also, naive Bayes has much lower cost than before (37->25) surpassing the Random Forest classifier. \n",
        "\n",
        "So far, we have some indications that SVM's fit nicely to this task but lets see how things are going to change when we  calibrate the probabilities."
      ],
      "metadata": {
        "id": "1snsbnRhiSh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibration on training set using Costcla's classifier\n",
        "np.random.seed(42)\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    \n",
        "    clf.fit(X_train, y_train)\n",
        "    y_train_prob = clf.predict_proba(X_train)\n",
        "    \n",
        "    bmr = BayesMinimumRiskClassifier(calibration=True)\n",
        "    bmr.fit(y_train.values.reshape(-1,1), y_train_prob)\n",
        "\n",
        "    y_test_prob = clf.predict_proba(X_test)\n",
        "    y_pred = bmr.predict(y_test_prob, cost_matrix)\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T \n",
        "    print(conf_m) \n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print(f\"\\nCost : {loss:.0f}\", end='\\n\\n')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "EGtThGSFqglR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58d53c2-5091-4eff-bcf4-0e65fba65c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.892     0.868     0.880        38\n",
            "    presence      0.839     0.867     0.852        30\n",
            "\n",
            "    accuracy                          0.868        68\n",
            "   macro avg      0.865     0.868     0.866        68\n",
            "weighted avg      0.868     0.868     0.868        68\n",
            "\n",
            "[[33  4]\n",
            " [ 5 26]]\n",
            "\n",
            "Cost : 25\n",
            "\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.963     0.684     0.800        38\n",
            "    presence      0.707     0.967     0.817        30\n",
            "\n",
            "    accuracy                          0.809        68\n",
            "   macro avg      0.835     0.825     0.808        68\n",
            "weighted avg      0.850     0.809     0.807        68\n",
            "\n",
            "[[26  1]\n",
            " [12 29]]\n",
            "\n",
            "Cost : 17\n",
            "\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.944     0.447     0.607        38\n",
            "    presence      0.580     0.967     0.725        30\n",
            "\n",
            "    accuracy                          0.676        68\n",
            "   macro avg      0.762     0.707     0.666        68\n",
            "weighted avg      0.784     0.676     0.659        68\n",
            "\n",
            "[[17  1]\n",
            " [21 29]]\n",
            "\n",
            "Cost : 26\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After calibrating the probabilities we can observe a slight improvement in the expected cost of the SVM and in the case of the Random Forest classifier. In the case of the SVM's which are known to overestimate low probabilities and underestimate high ones, we can see from the confusion matrix that we have less false positives than before and more true positives. Also in the naive Bayes model which is also known for producing inaccurate probabilities, we have just one false negative at the expense of higher false positives, which are not as bad as false negatives but ideally we would like that number to be lower. Random forest ensemble method emphasizes on detecting the true positives, and also having low false positives although 5 examples are misclassified as negatives which contribute to the higher cost."
      ],
      "metadata": {
        "id": "d5Wj1v8-jy8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid calibration\n",
        "np.random.seed(42)\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    calib_clf = CalibratedClassifierCV(base_estimator=clf, method='sigmoid',cv=5)\n",
        "    \n",
        "    calib_clf.fit(X_train, y_train)\n",
        "\n",
        "    y_test_prob = calib_clf.predict_proba(X_test)\n",
        "    y_pred = BayesMinimumRiskClassifier(calibration=False).predict(y_test_prob, cost_matrix)\n",
        "\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T \n",
        "    print(conf_m) \n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print(f\"\\nCost : {loss:.0f}\", end='\\n\\n')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "SejGh7wYto8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c272bacc-2f86-4264-a375-e1872a4e7149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.938     0.395     0.556        38\n",
            "    presence      0.558     0.967     0.707        30\n",
            "\n",
            "    accuracy                          0.647        68\n",
            "   macro avg      0.748     0.681     0.631        68\n",
            "weighted avg      0.770     0.647     0.623        68\n",
            "\n",
            "[[15  1]\n",
            " [23 29]]\n",
            "\n",
            "Cost : 28\n",
            "\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      1.000     0.395     0.566        38\n",
            "    presence      0.566     1.000     0.723        30\n",
            "\n",
            "    accuracy                          0.662        68\n",
            "   macro avg      0.783     0.697     0.644        68\n",
            "weighted avg      0.809     0.662     0.635        68\n",
            "\n",
            "[[15  0]\n",
            " [23 30]]\n",
            "\n",
            "Cost : 23\n",
            "\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.000     0.000     0.000        38\n",
            "    presence      0.441     1.000     0.612        30\n",
            "\n",
            "    accuracy                          0.441        68\n",
            "   macro avg      0.221     0.500     0.306        68\n",
            "weighted avg      0.195     0.441     0.270        68\n",
            "\n",
            "[[ 0  0]\n",
            " [38 30]]\n",
            "\n",
            "Cost : 38\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we make use of the Platt scaling technique to calibrate the probabilities. At first glance, we can see an increase in cost across all classifiers. Although this method managed to deal with the false negatives by minimizing them on all 3 classifiers, it seems a bit too aggresive, especially in the case of naive Bayes, where cost minimization is achieved by classifying all the test set examples as negative which of course results in a recall value of 0 in the case of the absence of heart disease. \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FmymHEZ-m5eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# isotonic calibration\n",
        "np.random.seed(42)\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    calib_clf = CalibratedClassifierCV(base_estimator=clf, method='isotonic',cv=5)\n",
        "    \n",
        "    calib_clf.fit(X_train, y_train)\n",
        "\n",
        "    y_test_prob = calib_clf.predict_proba(X_test)\n",
        "    y_pred = BayesMinimumRiskClassifier(calibration=False).predict(y_test_prob, cost_matrix)\n",
        "\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T \n",
        "    print(conf_m) \n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print(f\"\\nCost : {loss:.0f}\", end='\\n\\n')\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "9i2EFiOlu2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ea977d-5f5f-4274-ad1c-dbcaa53acebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.955     0.553     0.700        38\n",
            "    presence      0.630     0.967     0.763        30\n",
            "\n",
            "    accuracy                          0.735        68\n",
            "   macro avg      0.792     0.760     0.732        68\n",
            "weighted avg      0.812     0.735     0.728        68\n",
            "\n",
            "[[21  1]\n",
            " [17 29]]\n",
            "\n",
            "Cost : 22\n",
            "\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      1.000     0.579     0.733        38\n",
            "    presence      0.652     1.000     0.789        30\n",
            "\n",
            "    accuracy                          0.765        68\n",
            "   macro avg      0.826     0.789     0.761        68\n",
            "weighted avg      0.847     0.765     0.758        68\n",
            "\n",
            "[[22  0]\n",
            " [16 30]]\n",
            "\n",
            "Cost : 16\n",
            "\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.913     0.553     0.689        38\n",
            "    presence      0.622     0.933     0.747        30\n",
            "\n",
            "    accuracy                          0.721        68\n",
            "   macro avg      0.768     0.743     0.718        68\n",
            "weighted avg      0.785     0.721     0.714        68\n",
            "\n",
            "[[21  2]\n",
            " [17 28]]\n",
            "\n",
            "Cost : 27\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isotinic regression is another method of calibrating the expected probabilities. Based on the output we can claim that it outperforms the Platt scaling method although it is not much better than the calibration achieved by the costcla library implementation. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CVYnf6A8oxws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rebalancing"
      ],
      "metadata": {
        "id": "OyRFJQm1wFzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Runtime needs to be restarted at this point to load a newer version of sklearn that is compatible with the imbalanced-learn library**"
      ],
      "metadata": {
        "id": "vnTCeesWpu4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn>=1.0 imbalanced-learn"
      ],
      "metadata": {
        "id": "jyE1VkbHyJQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "n3s3kKPQwk2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "header_ = ['age', 'sex', 'chest_pain', 'resting_bp', 'cholesterol', 'blood_sugar', \n",
        "           'resting_ekg', 'max_hr', 'angina', 'oldpeak', 'slope_ST', 'no_of_vessels', 'thal', 'heart_disease']\n",
        "\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat',\n",
        "                 names=header_, header=None, delim_whitespace=True)\n",
        "\n",
        "'''\n",
        "  subtract 1 from the response variable so that 0 indicates absense and 1\n",
        "  presense of heart condition instead of the original 1 and 2 respectively\n",
        "'''\n",
        "\n",
        "df.heart_disease = df.heart_disease - 1\n",
        "\n",
        "df.heart_disease.value_counts() # 0=absence, 1=presence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed569c0-d421-4bf8-e388-309652144d6e",
        "id": "pqgbkvE7qTwt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    150\n",
              "1    120\n",
              "Name: heart_disease, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Undersample majority class (absence)\n",
        "\n",
        "X, y = df.drop(columns='heart_disease'), df.heart_disease\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "fp = np.full((y_test.shape[0],1), 1)\n",
        "fn = np.full((y_test.shape[0],1), 5)\n",
        "tp = np.zeros((y_test.shape[0],1))\n",
        "tn = np.zeros((y_test.shape[0],1))\n",
        "\n",
        "cost_matrix = np.hstack((fp, fn, tp, tn))\n",
        "\n",
        "# rows are the predicted values, columns the actual like in the slides\n",
        "cost_matrix_uci = np.array(((0,5),(1,0)))\n",
        "\n",
        "classifiers = [RandomForestClassifier(n_estimators=100), \n",
        "               SVC(kernel='linear', C=1, probability=True), MultinomialNB()]\n",
        "\n",
        "sampler = RandomUnderSampler(sampling_strategy='majority')\n",
        "\n",
        "print('before', Counter(y_train))\n",
        "X_train_rs, y_train_rs = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "print('after', Counter(y_train_rs), end='\\n\\n') # both classes have equal number of samples\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    clf.fit(X_train_rs, y_train_rs)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
        "    print(conf_m) \n",
        "    print(f\"Cost : {np.sum(conf_m * cost_matrix_uci)}\")\n",
        "    print('-' * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7BKTmLMwE5P",
        "outputId": "a702554b-76ca-4c44-f4eb-3713ca8a2a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before Counter({0: 112, 1: 90})\n",
            "after Counter({0: 90, 1: 90})\n",
            "\n",
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.912     0.816     0.861        38\n",
            "    presence      0.794     0.900     0.844        30\n",
            "\n",
            "    accuracy                          0.853        68\n",
            "   macro avg      0.853     0.858     0.852        68\n",
            "weighted avg      0.860     0.853     0.853        68\n",
            "\n",
            "[[31  3]\n",
            " [ 7 27]]\n",
            "Cost : 22\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.941     0.842     0.889        38\n",
            "    presence      0.824     0.933     0.875        30\n",
            "\n",
            "    accuracy                          0.882        68\n",
            "   macro avg      0.882     0.888     0.882        68\n",
            "weighted avg      0.889     0.882     0.883        68\n",
            "\n",
            "[[32  2]\n",
            " [ 6 28]]\n",
            "Cost : 16\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.833     0.789     0.811        38\n",
            "    presence      0.750     0.800     0.774        30\n",
            "\n",
            "    accuracy                          0.794        68\n",
            "   macro avg      0.792     0.795     0.793        68\n",
            "weighted avg      0.797     0.794     0.795        68\n",
            "\n",
            "[[30  6]\n",
            " [ 8 24]]\n",
            "Cost : 38\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use another technique, rebalancing, to alter the distributions of the classes either by undersampling the majority or by oversampling the minority class. \n",
        "\n",
        "In the case of undersampling here we randomely choose samples (without replacement) from the majority class so that we end up with the same number of training examples from both classes (90 positives - 90 negatives). In the cases of Random forest and SVM the costs are similar to the Isotinic regression method but in the case of naive Bayes the 6 negative misclassifications drive the total expected cost up."
      ],
      "metadata": {
        "id": "3t15S4w3rNK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversample \n",
        "np.random.seed(42)\n",
        "\n",
        "sampler = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "print('before', Counter(y_train))\n",
        "X_train_rs, y_train_rs = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "print('after', Counter(y_train_rs), end='\\n\\n') # both classes have equal number of samples\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    clf.fit(X_train_rs, y_train_rs)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
        "    print(conf_m) \n",
        "    print(f\"Cost : {np.sum(conf_m * cost_matrix_uci)}\")\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiXnneUW1d7W",
        "outputId": "782708a5-aae9-494d-9bea-45344adfd9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before Counter({0: 112, 1: 90})\n",
            "after Counter({0: 112, 1: 112})\n",
            "\n",
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.886     0.816     0.849        38\n",
            "    presence      0.788     0.867     0.825        30\n",
            "\n",
            "    accuracy                          0.838        68\n",
            "   macro avg      0.837     0.841     0.837        68\n",
            "weighted avg      0.843     0.838     0.839        68\n",
            "\n",
            "[[31  4]\n",
            " [ 7 26]]\n",
            "Cost : 27\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.939     0.816     0.873        38\n",
            "    presence      0.800     0.933     0.862        30\n",
            "\n",
            "    accuracy                          0.868        68\n",
            "   macro avg      0.870     0.875     0.867        68\n",
            "weighted avg      0.878     0.868     0.868        68\n",
            "\n",
            "[[31  2]\n",
            " [ 7 28]]\n",
            "Cost : 17\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.857     0.789     0.822        38\n",
            "    presence      0.758     0.833     0.794        30\n",
            "\n",
            "    accuracy                          0.809        68\n",
            "   macro avg      0.807     0.811     0.808        68\n",
            "weighted avg      0.813     0.809     0.809        68\n",
            "\n",
            "[[30  5]\n",
            " [ 8 25]]\n",
            "Cost : 33\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use oversampling instead to increase the number of examples of the minority class to be the same as the majority. Again, SVM is very consistent on delivering low cost followed by the Random forest classifier, which sees a increase in expected cost. Oversampling has benefited the naive Bayes classifier by lowering the expected cost from 38 to 33 although it is still considerably worse that the other 2.\n",
        "\n",
        "As a note on oversampling-undersampling we can point out that naive Bayes, which is an method that heavily relies on good probabilities, does not benefit from stratification. Next, we'll try to combat that by combining the 2 methods (oversampling & undersampling)"
      ],
      "metadata": {
        "id": "MFwblSDBs_2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the 2 methods\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "sampler1 = RandomUnderSampler(sampling_strategy='majority')\n",
        "sampler2 = RandomOverSampler(sampling_strategy={0:120, 1:250})\n",
        "\n",
        "print('before', Counter(y_train))\n",
        "X_train_rs, y_train_rs = sampler1.fit_resample(X_train, y_train)\n",
        "\n",
        "print('after under sample', Counter(y_train_rs)) # both classes have equal number of samples\n",
        "\n",
        "X_train_rs2, y_train_rs2 = sampler2.fit_resample(X_train_rs, y_train_rs)\n",
        "\n",
        "print('after oversample', Counter(y_train_rs2), end='\\n\\n') # both classes have equal number of samples\n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    clf.fit(X_train_rs2, y_train_rs2)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
        "    print(conf_m) \n",
        "    print(f\"Cost : {np.sum(conf_m * cost_matrix_uci)}\")\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsKbl1VM2dhi",
        "outputId": "d89d231f-5490-4925-f894-d7c859691f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before Counter({0: 112, 1: 90})\n",
            "after under sample Counter({0: 90, 1: 90})\n",
            "after oversample Counter({1: 250, 0: 120})\n",
            "\n",
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.903     0.737     0.812        38\n",
            "    presence      0.730     0.900     0.806        30\n",
            "\n",
            "    accuracy                          0.809        68\n",
            "   macro avg      0.816     0.818     0.809        68\n",
            "weighted avg      0.827     0.809     0.809        68\n",
            "\n",
            "[[28  3]\n",
            " [10 27]]\n",
            "Cost : 25\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.935     0.763     0.841        38\n",
            "    presence      0.757     0.933     0.836        30\n",
            "\n",
            "    accuracy                          0.838        68\n",
            "   macro avg      0.846     0.848     0.838        68\n",
            "weighted avg      0.857     0.838     0.838        68\n",
            "\n",
            "[[29  2]\n",
            " [ 9 28]]\n",
            "Cost : 19\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.909     0.789     0.845        38\n",
            "    presence      0.771     0.900     0.831        30\n",
            "\n",
            "    accuracy                          0.838        68\n",
            "   macro avg      0.840     0.845     0.838        68\n",
            "weighted avg      0.848     0.838     0.839        68\n",
            "\n",
            "[[30  3]\n",
            " [ 8 27]]\n",
            "Cost : 23\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After combining the 2 sampling strategies, first by undersampling the majority class and then oversampling the minority so that it has approximately twice the samples of the majority, we can see a massive decrease in the expected cost of the naive Bayes classifier (from over 33 cost down to 23). So by putting more emphasis on the class with the highest cost the classifier has managed to reduce the false negatives. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1a3Kf3_vvRYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighting"
      ],
      "metadata": {
        "id": "2xB3_BdO5Les"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight each example based on its misclassification cost.\n",
        "\n",
        "If the response variable for an example is 0 (absence) weight it with 1, otherwise weight is equal to 5"
      ],
      "metadata": {
        "id": "V4PdF8OuxUIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "weights = np.zeros_like(y_train)\n",
        "weights[np.where(y_train == 0)] = 1\n",
        "weights[np.where(y_train == 1)] = 5 \n",
        "\n",
        "for clf in classifiers:\n",
        "    print(clf.__class__.__name__)\n",
        "    clf.fit(X_train, y_train, weights)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                digits=3, target_names=['absence', 'presence']))\n",
        "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
        "    print(conf_m) \n",
        "    print(f\"\\nCost w/ weighting : {np.sum(conf_m * cost_matrix_uci)}\")\n",
        "    print('-' * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sISBS3A65K9m",
        "outputId": "b3d1d5c4-6c7d-4f1a-9e97-f95860c05488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.846     0.868     0.857        38\n",
            "    presence      0.828     0.800     0.814        30\n",
            "\n",
            "    accuracy                          0.838        68\n",
            "   macro avg      0.837     0.834     0.835        68\n",
            "weighted avg      0.838     0.838     0.838        68\n",
            "\n",
            "[[33  6]\n",
            " [ 5 24]]\n",
            "\n",
            "Cost w/ weighting : 35\n",
            "--------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.926     0.658     0.769        38\n",
            "    presence      0.683     0.933     0.789        30\n",
            "\n",
            "    accuracy                          0.779        68\n",
            "   macro avg      0.804     0.796     0.779        68\n",
            "weighted avg      0.819     0.779     0.778        68\n",
            "\n",
            "[[25  2]\n",
            " [13 28]]\n",
            "\n",
            "Cost w/ weighting : 23\n",
            "--------------------------------------------------\n",
            "MultinomialNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence      0.903     0.737     0.812        38\n",
            "    presence      0.730     0.900     0.806        30\n",
            "\n",
            "    accuracy                          0.809        68\n",
            "   macro avg      0.816     0.818     0.809        68\n",
            "weighted avg      0.827     0.809     0.809        68\n",
            "\n",
            "[[28  3]\n",
            " [10 27]]\n",
            "\n",
            "Cost w/ weighting : 25\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see a noticable increase in the expected cost of the Random forest and the SVM classifiers, much worse than the 2 other methods (calibration, rebalancing). Naive Bayes seems to be the one that mostly benefits from this method since the expected cost is much lower than doing no cost-minimzation at all and on par with calibration/rebalancing (w/ combination of sampling)."
      ],
      "metadata": {
        "id": "aklvpa-Tzich"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We can conclude by saying that Support Vector Machines consistently outperformed the other 2 algorithms on almost all instances on this particular dataset. They delivered the lowest expected cost and didn't seem to be very sensitive to the method used whether it was calibration, rebalancing or weighting (although they didn't perform great here, it was still the best among the 3). Random forests didn't benefit greatly from calibration since the probabilities they emit are very accurate. Some of their best results were achieved using isotonic regression and undersampling although still were not far off the baseline (without cost minimization). Naive Bayes was really affected by calibration since the probabilities it outputs are not very accurate."
      ],
      "metadata": {
        "id": "dcDGncFh01Sb"
      }
    }
  ]
}